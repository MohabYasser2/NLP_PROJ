# ğŸ“Š COMPLETE SYSTEM ARCHITECTURE & DATA FLOW

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ARABIC DIACRITIZATION SYSTEM                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      INPUT DATA LAYER               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                     â”‚
â”‚  data/train.txt (50,001 lines)     â”‚
â”‚  "ÙˆÙÙ„ÙÙˆÙ’ Ø¬ÙÙ…ÙØ¹Ù Ø«ÙÙ…ÙÙ‘ Ø¹ÙÙ„ÙÙ…Ù..."   â”‚
â”‚  (Diacritized Arabic text)          â”‚
â”‚                                     â”‚
â”‚  data/val.txt                       â”‚
â”‚  (Validation data)                  â”‚
â”‚                                     â”‚
â”‚  utils/diacritic2id.pickle          â”‚
â”‚  {'' â†’ 0, 'Ù‹' â†’ 1, ..., 'Ù‘' â†’ 14}  â”‚
â”‚  (15 diacritic classes)             â”‚
â”‚                                     â”‚
â”‚  utils/vocab.py                     â”‚
â”‚  (Character vocabulary builder)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           PREPROCESSING LAYER                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                      â”‚                          â”‚
â”‚  tokenize.py                         â”‚  encode_labels.py        â”‚
â”‚  â€¢ Separates base chars              â”‚  â€¢ Maps diacritics       â”‚
â”‚  â€¢ Extracts diacritics               â”‚  â€¢ Uses diacritic2id     â”‚
â”‚  â€¢ Creates 1-to-1 alignment          â”‚  â€¢ Outputs IDs (0-14)    â”‚
â”‚                                      â”‚                          â”‚
â”‚  Input: Raw text                     â”‚  Input: Diacritic lists  â”‚
â”‚  Output: X (chars), Y (diacs)        â”‚  Output: Y_encoded (IDs) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                â”‚               â”‚
         â”‚                â”‚               â”‚
         â–¼                â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FEATURE EXTRACTION LAYER                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Option A: CHARACTER EMBEDDINGS (CPU)                          â”‚
â”‚  â€¢ vocab.encode() â†’ char IDs                                   â”‚
â”‚  â€¢ Embedding(char_ids) â†’ 100-dim vectors                      â”‚
â”‚  â€¢ Shape: (batch, seq_len, 100)                                â”‚
â”‚                                                                 â”‚
â”‚  Option B: CONTEXTUAL EMBEDDINGS (GPU)                         â”‚
â”‚  â€¢ contextual_embeddings.py â†’ AraBERT                          â”‚
â”‚  â€¢ Forward pass through BERT                                   â”‚
â”‚  â€¢ Extract per-character embeddings                            â”‚
â”‚  â€¢ Shape: (batch, seq_len, 768)                                â”‚
â”‚                                                                 â”‚
â”‚  pad_sequences.py                                              â”‚
â”‚  â€¢ Pad to same length                                          â”‚
â”‚  â€¢ Create attention masks                                      â”‚
â”‚  â€¢ Handle variable lengths                                     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                          â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MODEL LAYER (BiLSTM-CRF)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ INPUT                                                     â”‚ â”‚
â”‚  â”‚ Shape: (batch_size, seq_length, embedding_dim)           â”‚ â”‚
â”‚  â”‚        (32, ~100, 100 or 768)                             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚              â”‚                                                  â”‚
â”‚              â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ EMBEDDING LAYER (if char embeddings)                      â”‚ â”‚
â”‚  â”‚ Embedding(vocab_size=160, embedding_dim=100, padding=0)   â”‚ â”‚
â”‚  â”‚ Already embedded if using contextual                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚              â”‚                                                  â”‚
â”‚              â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ BiLSTM LAYER                                              â”‚ â”‚
â”‚  â”‚ Input:  (batch, seq_len, embedding_dim)                  â”‚ â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚ â”‚ Forward LSTM (hidden=128)  â†’ 128 dims              â”‚  â”‚ â”‚
â”‚  â”‚ â”‚ Backward LSTM (hidden=128) â†’ 128 dims              â”‚  â”‚ â”‚
â”‚  â”‚ â”‚ Concatenate â†’ 256 dims                             â”‚  â”‚ â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â”‚ Output: (batch, seq_len, 256)                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚              â”‚                                                  â”‚
â”‚              â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ DENSE LAYER                                               â”‚ â”‚
â”‚  â”‚ Linear(in=256, out=15)                                    â”‚ â”‚
â”‚  â”‚ Maps to 15 diacritic classes                              â”‚ â”‚
â”‚  â”‚ Output: (batch, seq_len, 15)                              â”‚ â”‚
â”‚  â”‚         = Emission scores for CRF                         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚              â”‚                                                  â”‚
â”‚              â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ CRF LAYER (Conditional Random Field)                      â”‚ â”‚
â”‚  â”‚ â€¢ Learns tag transition probabilities                    â”‚ â”‚
â”‚  â”‚ â€¢ Viterbi decoding for inference                         â”‚ â”‚
â”‚  â”‚ â€¢ CRF Loss for training                                  â”‚ â”‚
â”‚  â”‚ Input: (batch, seq_len, 15) emissions + tags + mask      â”‚ â”‚
â”‚  â”‚ Output: Sequence predictions OR loss value               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚              â”‚                                                  â”‚
â”‚              â–¼                                                  â”‚
â”‚  OUTPUT                                                         â”‚
â”‚  Training: Loss scalar (backpropagation)                        â”‚
â”‚  Inference: List of predictions (0-14 for each char)           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TRAINING LOOP                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  For epoch in range(100):                                      â”‚
â”‚    â”œâ”€ Train Phase:                                             â”‚
â”‚    â”‚  â”œâ”€ Load batch (X, Y, mask)                              â”‚
â”‚    â”‚  â”œâ”€ Forward pass: pred = model(X, tags=Y, mask=mask)    â”‚
â”‚    â”‚  â”œâ”€ Loss = CRF_loss(emissions, tags, mask)              â”‚
â”‚    â”‚  â”œâ”€ Backward: loss.backward()                            â”‚
â”‚    â”‚  â”œâ”€ Clip gradients (max_norm=5.0)                        â”‚
â”‚    â”‚  â””â”€ Update: optimizer.step()                             â”‚
â”‚    â”‚                                                           â”‚
â”‚    â”œâ”€ Validation Phase:                                       â”‚
â”‚    â”‚  â”œâ”€ Forward pass: pred = model(X, mask=mask)            â”‚
â”‚    â”‚  â”œâ”€ Calculate DER & Accuracy                             â”‚
â”‚    â”‚  â””â”€ Check if best model                                  â”‚
â”‚    â”‚                                                           â”‚
â”‚    â”œâ”€ Early Stopping Check:                                   â”‚
â”‚    â”‚  â”œâ”€ If DER improved: save model, reset patience          â”‚
â”‚    â”‚  â””â”€ If DER plateaued: patience--                         â”‚
â”‚    â”‚     If patience == 0: STOP                               â”‚
â”‚    â”‚                                                           â”‚
â”‚    â””â”€ Learning rate scheduling                                â”‚
â”‚       â””â”€ Decrease LR every 10 epochs (gamma=0.5)              â”‚
â”‚                                                                 â”‚
â”‚  CONFIG:                                                       â”‚
â”‚  â€¢ Batch size: 32                                              â”‚
â”‚  â€¢ Learning rate: 0.001 (Adam)                                â”‚
â”‚  â€¢ Weight decay: 1e-5                                          â”‚
â”‚  â€¢ Gradient clip: 5.0                                          â”‚
â”‚  â€¢ Epochs: 100 (max)                                           â”‚
â”‚  â€¢ Patience: 10                                                â”‚
â”‚  â€¢ Optimizer: Adam                                             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           EVALUATION & METRICS                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  DER (Diacritic Error Rate)                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚  Errors per character (excluding padding)                      â”‚
â”‚  DER = Errors / Total_characters                               â”‚
â”‚  Expected: 5-15%                                               â”‚
â”‚                                                                 â”‚
â”‚  Accuracy                                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚  Correct predictions / Total (excluding padding)               â”‚
â”‚  Accuracy = (1 - DER)                                          â”‚
â”‚  Expected: 85-95%                                              â”‚
â”‚                                                                 â”‚
â”‚  Loss Curves                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â”‚
â”‚  â€¢ Training loss: should decrease monotonically                â”‚
â”‚  â€¢ Validation loss: should decrease then plateau               â”‚
â”‚  â€¢ Watch for overfitting                                       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            OUTPUT & DEPLOYMENT                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  models/best_bilstm_crf.pth                                    â”‚
â”‚  â€¢ Saved state_dict of trained model                           â”‚
â”‚  â€¢ Can load with torch.load()                                  â”‚
â”‚  â€¢ Ready for inference                                         â”‚
â”‚                                                                 â”‚
â”‚  Usage:                                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€                                                        â”‚
â”‚  model = BiLSTMCRF(...)                                        â”‚
â”‚  model.load_state_dict(torch.load('models/best_bilstm_crf')) â”‚
â”‚  model.eval()                                                  â”‚
â”‚                                                                 â”‚
â”‚  predictions = model(X_test, mask=mask)                        â”‚
â”‚  # predictions: list of diacritic IDs for each char            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Format Examples

### Input Format

```
Raw Text Line:
ÙˆÙÙ„ÙÙˆÙ’ Ø¬ÙÙ…ÙØ¹Ù Ø«ÙÙ…ÙÙ‘

After Tokenization:
Characters: ['Ùˆ', 'Ù„', 'Ùˆ', 'Ø¬', 'Ù…', 'Ø¹', 'Ø«', 'Ù…']
Diacritics: ['Ù', 'Ù’', '', 'Ù', 'Ù', 'Ù', 'Ù', '']
```

### Processing Example

```
Character 'Ùˆ' (waw)
    â†“
char_id = vocab.char2id['Ùˆ'] = 42 (example)
    â†“
Diacritic: 'Ù' (Fatha)
    â†“
diacritic_id = diacritic2id['Ù'] = 3 (example)
    â†“
Model learns: char_42 â†’ diacritic_3
```

### Embedding Shapes

```
Character Embeddings (Batch of 32):
Input:  (32, 100)       # 32 chars, 100-dim embeddings
Output: (32, 100)

BiLSTM (Batch of 32 with 100 seq length):
Input:  (32, 100, 100)  # batch, seq_len, embedding_dim
LSTM:   (32, 100, 256)  # batch, seq_len, hidden_dim (bidirectional)
Output: (32, 100, 256)  # Same shape

CRF Emissions:
Input:  (32, 100, 256)  # from BiLSTM
Dense:  (32, 100, 15)   # mapped to 15 classes
CRF:    Loss or predictions
```

---

## Key Statistics

### Data

- Training samples: 50,001
- Validation samples: ~6,000 (10%)
- Diacritic classes: 15
- Character vocabulary: ~160 unique chars
- Average sequence length: ~80 chars

### Model

- Embedding dimension: 100 (char) or 768 (AraBERT)
- Hidden dimension: 256 (128 per direction)
- BiLSTM layers: 1
- Output classes: 15
- Total parameters: ~800K - 2M

### Training

- Batch size: 32
- Learning rate: 0.001
- Max epochs: 100
- Early stopping patience: 10
- Gradient clipping: 5.0

### Performance

- Training speed (CPU): 15-20 min/epoch
- Training speed (GPU): 5-8 min/epoch
- Expected accuracy: 85-95%
- Expected DER: 5-15%

---

## âœ… Complete System Verified

All components are:

- âœ… Correctly implemented
- âœ… Properly configured
- âœ… Using official diacritic mappings
- âœ… Following best practices
- âœ… Ready for production training

**The entire system is ready to train!** ğŸš€
